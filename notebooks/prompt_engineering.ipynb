{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install -U langchain openai langchain-openai langchain-experimental python-dotenv faiss-cpu langchain-community langchainhub langgraph"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75519fd952da5371",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Zero-shot"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f55e0d1cc977d67d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "response = llm.invoke(\n",
    "    \"\"\"Translate the following sentence to Russian:\n",
    "    Hello world!\n",
    "\"\"\").content\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac64c2f8047f742d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Few-shot"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e2c3dc083bf47df"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "response = llm.invoke(\n",
    "    \"\"\"The sentence \"Hello, people of Rivendell!\" in Sindarin is \"Suilad, nÃºr -o Rivendell!\"\n",
    "    The sentence \"Conquer the world.\" in Sindarin is \"Ortheri- i ambar.\"\n",
    "    Translate the following sentence to Sindarin:\n",
    "    Hello world!\n",
    "\"\"\").content\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f4e92b92ddd8eeb",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Chain-of-thought"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3242b8281582c7e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, model_name=\"gpt-3.5-turbo\")\n",
    "response = llm.invoke(\n",
    "    \"\"\"Which is a faster way to get to work?\n",
    "Option 1: Take a 1000 minute bus, then a half hour train, and finally a 10 minute bike ride.\n",
    "Option 2: Take an 800 minute bus, then an hour train, and finally a 30 minute bike ride.\n",
    "\"\"\").content\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c64ab25ad63f2d10",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "response = llm.invoke(\n",
    "    \"\"\"Which is a faster way to get home?\n",
    "Option 1: Take an 10 minutes bus, then an 40 minute bus, and finally a 10 minute train.\n",
    "Option 2: Take a 90 minutes train, then a 45 minute bike ride, and finally a 10 minute bus.\n",
    "Option 1 will take 10+40+10 = 60 minutes.\n",
    "Option 2 will take 90+45+10=145 minutes.\n",
    "Since Option 1 takes 60 minutes and Option 2 takes 145 minutes, Option 1 is faster.\n",
    "\n",
    "Which is a faster way to get to work?\n",
    "Option 1: Take a 1000 minute bus, then a half hour train, and finally a 10 minute bike ride.\n",
    "Option 2: Take an 800 minute bus, then an hour train, and finally a 30 minute bike ride.\n",
    "\"\"\").content\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7810581474ea3da1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Self-Consistency"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b15be57ec2335052"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, model_name=\"gpt-3.5-turbo\")\n",
    "prompt = \"\"\"Which is a faster way to get to work?\n",
    "Option 1: Take a 1000 minute bus, then a half hour train, and finally a 10 minute bike ride.\n",
    "Option 2: Take an 800 minute bus, then an hour train, and finally a 30 minute bike ride.\n",
    "\"\"\"\n",
    "results = [llm.invoke(prompt).content for _ in range(3)]\n",
    "print(results)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "348019b54cd0f06f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt = f\"Given the following hypotheses, please give the final result based on consensus: {results}\"\n",
    "response = llm.invoke(prompt).content\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efbe37023577240d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Generated Knowledge"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b899f1fc8cd14875"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, model_name=\"gpt-3.5-turbo\")\n",
    "response = llm.invoke(\n",
    "    \"\"\"Part of golf is trying to get a higher point total than others. Yes or No?\n",
    "\"\"\").content\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56e8b6c27feb260d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "response = llm.invoke(\n",
    "    \"\"\"Step 1: Generate 10 facts about rules of golf.\n",
    "    Step 2: Part of golf is trying to get a higher point total than others. Yes or No?\n",
    "\"\"\").content\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89b31fa6db53450f",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Prompt Chaining"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a274af222600466"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "with open('../data/doctor_who.txt', 'r') as file:\n",
    "    doctors_text = file.read()\n",
    "\n",
    "doctors = llm.invoke(\n",
    "    f\"\"\"Based on the text provided below in triple quotes please list all the actors who have been playing in the Doctor Who with their genders. Please put this list into json format.\n",
    "    '''{doctors_text}'''\n",
    "\"\"\").content\n",
    "print(doctors)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb2f8d1694b08235",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "response = llm.invoke(f\"\"\"Based on the list of actors in the following json please identify the actresses:\n",
    "{doctors}\"\"\").content\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25be73254f3592fa",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tree of Thoughts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a42a060631f470e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=1, max_tokens=512, model=\"gpt-4-turbo-preview\")\n",
    "\n",
    "sudoku_puzzle = \"3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1\"\n",
    "sudoku_solution = \"3,4,1,2|1,2,3,4|2,1,4,3|4,3,2,1\"\n",
    "problem_description = f\"\"\"\n",
    "{sudoku_puzzle}\n",
    "\n",
    "- This is a 4x4 Sudoku puzzle.\n",
    "- The * represents a cell to be filled.\n",
    "- The | character separates rows.\n",
    "- At each step, replace one or more * with digits 1-4.\n",
    "- There must be no duplicate digits in any row, column or 2x2 subgrid.\n",
    "- Keep the known digits from previous valid thoughts in place.\n",
    "- Each thought can be a partial or the final solution.\n",
    "- Make sure that the output format for each thought is correct.\n",
    "- Make sure your responses are consistent in terms of the format.\n",
    "- Make sure that you follow the instructions very strictly!\n",
    "\"\"\".strip()\n",
    "print(problem_description)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2dbb58988b207e2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Tuple\n",
    "\n",
    "from langchain_experimental.tot.checker import ToTChecker\n",
    "from langchain_experimental.tot.thought import ThoughtValidity\n",
    "\n",
    "\n",
    "class MyChecker(ToTChecker):\n",
    "    def evaluate(\n",
    "        self, problem_description: str, thoughts: Tuple[str, ...] = ()\n",
    "    ) -> ThoughtValidity:\n",
    "        last_thought = thoughts[-1]\n",
    "        clean_solution = last_thought.replace(\" \", \"\").replace('\"', \"\")\n",
    "        regex_solution = clean_solution.replace(\"*\", \".\").replace(\"|\", \"\\\\|\")\n",
    "        if sudoku_solution in clean_solution:\n",
    "            return ThoughtValidity.VALID_FINAL\n",
    "        elif re.search(regex_solution, sudoku_solution):\n",
    "            return ThoughtValidity.VALID_INTERMEDIATE\n",
    "        else:\n",
    "            return ThoughtValidity.INVALID"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f7bed2c848ddfc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_experimental.tot.base import ToTChain\n",
    "\n",
    "tot_chain = ToTChain(\n",
    "    llm=llm, checker=MyChecker(), k=30, c=5, verbose=True, verbose_llm=False\n",
    ")\n",
    "tot_chain.invoke({\"problem_description\": problem_description})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15bdff1f7b85af06",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Retrieval Augmented Generation (RAG)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da9f1b632870df43"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"Alice works at Deutsche Bank\",\n",
    "     \"Mike plays football\",\n",
    "     \"Lucy likes to read books\"], \n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain.invoke(\"Where does Alice work?\"))\n",
    "print(chain.invoke(\"What does Alice like?\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "654381b59acfce54",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Automatic Reasoning and Tool Use (ART)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "161f4aee12d39b26"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "tool = Tool(\n",
    "    name=\"google_search\",\n",
    "    description=\"Search Google for recent results.\",\n",
    "    func=search.run,\n",
    ")\n",
    "tools = [tool]\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "agent_executor.invoke({\"input\": \"What's the latest news on Deutsche Bank layoffs?\"})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15979fda66fc9740",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### ReAct"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "faa553509987af22"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import create_react_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "tool = Tool(\n",
    "    name=\"google_search\",\n",
    "    description=\"Search Google for recent results.\",\n",
    "    func=search.run,\n",
    ")\n",
    "tools = [tool]\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "agent_executor.invoke({\"input\": \"What's the square root of the age J. R. R. Tolkien died at?\"})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "794d49c862bf3d2c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Reflexion"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80dc29a6bafaefb0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "\n",
    "search = TavilySearchAPIWrapper()\n",
    "tavily_tool = TavilySearchResults(api_wrapper=search, max_results=5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e231116782bfff39",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List\n",
    "\n",
    "from langchain.output_parsers.openai_tools import (\n",
    "    JsonOutputToolsParser,\n",
    "    PydanticToolsParser,\n",
    ")\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\n",
    "\n",
    "# This a helper class we have that is useful for running tools\n",
    "# It takes in an agent action and calls that tool and returns the result\n",
    "tool_executor = ToolExecutor([tavily_tool])\n",
    "# Parse the tool messages for the execution / invocation\n",
    "parser = JsonOutputToolsParser(return_id=True)\n",
    "\n",
    "\n",
    "def execute_tools(state: List[BaseMessage]) -> List[BaseMessage]:\n",
    "    tool_invocation: AIMessage = state[-1]\n",
    "    parsed_tool_calls = parser.invoke(tool_invocation)\n",
    "    ids = []\n",
    "    tool_invocations = []\n",
    "    for parsed_call in parsed_tool_calls:\n",
    "        for query in parsed_call[\"args\"][\"search_queries\"]:\n",
    "            tool_invocations.append(\n",
    "                ToolInvocation(\n",
    "                    # We only have this one for now. Would want to map it\n",
    "                    # if we change\n",
    "                    tool=\"tavily_search_results_json\",\n",
    "                    tool_input=query,\n",
    "                )\n",
    "            )\n",
    "            ids.append(parsed_call[\"id\"])\n",
    "\n",
    "    outputs = tool_executor.batch(tool_invocations)\n",
    "    outputs_map = defaultdict(dict)\n",
    "    for id_, output, invocation in zip(ids, outputs, tool_invocations):\n",
    "        outputs_map[id_][invocation.tool_input] = output\n",
    "\n",
    "    return [\n",
    "        ToolMessage(content=json.dumps(query_outputs), tool_call_id=id_)\n",
    "        for id_, query_outputs in outputs_map.items()\n",
    "    ]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d0181a36aed83da",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, ValidationError\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import traceable\n",
    "\n",
    "actor_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are expert researcher.\n",
    "Current time: {time}\n",
    "\n",
    "1. {first_instruction}\n",
    "2. Reflect and critique your answer. Be severe to maximize improvement.\n",
    "3. Recommend search queries to research information and improve your answer.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"system\", \"Answer the user's question above using the required format.\"),\n",
    "    ]\n",
    ").partial(\n",
    "    time=lambda: datetime.datetime.now().isoformat(),\n",
    ")\n",
    "\n",
    "\n",
    "class Reflection(BaseModel):\n",
    "    missing: str = Field(description=\"Critique of what is missing.\")\n",
    "    superfluous: str = Field(description=\"Critique of what is superfluous\")\n",
    "\n",
    "\n",
    "class AnswerQuestion(BaseModel):\n",
    "    \"\"\"Answer the question.\"\"\"\n",
    "\n",
    "    answer: str = Field(description=\"~250 word detailed answer to the question.\")\n",
    "    reflection: Reflection = Field(description=\"Your reflection on the initial answer.\")\n",
    "    search_queries: List[str] = Field(\n",
    "        description=\"1-3 search queries for researching improvements to address the critique of your current answer.\"\n",
    "    )\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k-0613\")\n",
    "initial_answer_chain = actor_prompt_template.partial(\n",
    "    first_instruction=\"Provide a detailed ~250 word answer.\"\n",
    ") | llm.bind_tools(tools=[AnswerQuestion], tool_choice=\"AnswerQuestion\")\n",
    "validator = PydanticToolsParser(tools=[AnswerQuestion])\n",
    "\n",
    "\n",
    "class ResponderWithRetries:\n",
    "    def __init__(self, runnable, validator):\n",
    "        self.runnable = runnable\n",
    "        self.validator = validator\n",
    "\n",
    "    @traceable\n",
    "    def respond(self, state: List[BaseMessage]):\n",
    "        response = []\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                response = self.runnable.invoke({\"messages\": state})\n",
    "                self.validator.invoke(response)\n",
    "                return response\n",
    "            except ValidationError as e:\n",
    "                state = state + [HumanMessage(content=repr(e))]\n",
    "        return response"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc20453df3f9b40b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "first_responder = ResponderWithRetries(\n",
    "    runnable=initial_answer_chain, validator=validator\n",
    ")\n",
    "example_question = \"Why is reflection useful in AI?\"\n",
    "initial = first_responder.respond([HumanMessage(content=example_question)])\n",
    "parsed = parser.invoke(initial)\n",
    "parsed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a96fe5e21f3461f7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "revise_instructions = \"\"\"Revise your previous answer using the new information.\n",
    "    - You should use the previous critique to add important information to your answer.\n",
    "        - You MUST include numerical citations in your revised answer to ensure it can be verified.\n",
    "        - Add a \"References\" section to the bottom of your answer (which does not count towards the word limit). In form of:\n",
    "            - [1] https://example.com\n",
    "            - [2] https://example.com\n",
    "    - You should use the previous critique to remove superfluous information from your answer and make SURE it is not more than 250 words.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Extend the initial answer schema to include references.\n",
    "# Forcing citation in the model encourages grounded responses\n",
    "class ReviseAnswer(AnswerQuestion):\n",
    "    \"\"\"Revise your original answer to your question.\"\"\"\n",
    "\n",
    "    references: List[str] = Field(\n",
    "        description=\"Citations motivating your updated answer.\"\n",
    "    )\n",
    "\n",
    "\n",
    "revision_chain = actor_prompt_template.partial(\n",
    "    first_instruction=revise_instructions\n",
    ") | llm.bind_tools(tools=[ReviseAnswer], tool_choice=\"ReviseAnswer\")\n",
    "revision_validator = PydanticToolsParser(tools=[ReviseAnswer])\n",
    "\n",
    "revisor = ResponderWithRetries(runnable=revision_chain, validator=revision_validator)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "298744bc3ff5d9f1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "revised = revisor.respond(\n",
    "    [\n",
    "        HumanMessage(content=\"\"),\n",
    "        initial,\n",
    "        ToolMessage(\n",
    "            tool_call_id=initial.additional_kwargs[\"tool_calls\"][0][\"id\"],\n",
    "            content=json.dumps(\n",
    "                tavily_tool.invoke(str(parsed[0][\"args\"][\"search_queries\"]))\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "parsed = parser.invoke(revised)\n",
    "parsed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70ea0b70a9a11a89",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langgraph.graph import END, MessageGraph\n",
    "\n",
    "MAX_ITERATIONS = 5\n",
    "builder = MessageGraph()\n",
    "builder.add_node(\"draft\", first_responder.respond)\n",
    "builder.add_node(\"execute_tools\", execute_tools)\n",
    "builder.add_node(\"revise\", revisor.respond)\n",
    "# draft -> execute_tools\n",
    "builder.add_edge(\"draft\", \"execute_tools\")\n",
    "# execute_tools -> revise\n",
    "builder.add_edge(\"execute_tools\", \"revise\")\n",
    "\n",
    "# Define looping logic:\n",
    "\n",
    "\n",
    "def _get_num_iterations(state: List[BaseMessage]):\n",
    "    i = 0\n",
    "    for m in state[::-1]:\n",
    "        if not isinstance(m, (ToolMessage, AIMessage)):\n",
    "            break\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "\n",
    "def event_loop(state: List[BaseMessage]) -> str:\n",
    "    # in our case, we'll just stop after N plans\n",
    "    num_iterations = _get_num_iterations(state)\n",
    "    if num_iterations > MAX_ITERATIONS:\n",
    "        return END\n",
    "    return \"execute_tools\"\n",
    "\n",
    "\n",
    "# revise -> execute_tools OR end\n",
    "builder.add_conditional_edges(\"revise\", event_loop)\n",
    "builder.set_entry_point(\"draft\")\n",
    "graph = builder.compile()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb41a2530badfe82",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "events = graph.stream(\n",
    "    [HumanMessage(content=\"How should we handle the climate crisis?\")]\n",
    ")\n",
    "for i, step in enumerate(events):\n",
    "    node, output = next(iter(step.items()))\n",
    "    print(f\"## {i+1}. {node}\")\n",
    "    print(str(output)[:100] + \" ...\")\n",
    "    print(\"---\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a8defec2b1293c5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(parser.invoke(step[END][-1])[0][\"args\"][\"answer\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3035a29bd5ffd215",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
